
A. Convolutional horses and humans

1\. Describe the ImageDataGenerator() command and its associated argument. What objects and arguments do you need to specify in order to flow from the directory to the generated object? What is the significance of specifying the target_size = as it relates to your source images of varying sizes? What considerations might you reference when programming the class mode = argument? How difference exists when applying the ImageDataGenerator() and .flow_from_directory() commands to the training and test datasets?

The ImageDataGenerator() command and the rescale argument creates an instance, which initializes an object of the ImageDataGenerator class, ready to be used. The rescale parameter that's passed in does an extra step for us so we don't have to do it--it automatically resizes the data that's passed in and multiplies it by that value (which is 1/255, in this case). By specifying this, it actually allows the data to be prepared and is better analyzed in training. By specifying target_size, we tell the function how to process and output the information it's given. This is useful because it allows there to be a set, consistent value, which not only helps the user but also helps the program run more consistently (prevents inconsistencies, e.g. errors). class_mode was mentioned to be one of the most common places for errors, because that's exactly where the program is told *how* to create its model. If you have 2 things to identify (such as human vs. horse or cat vs. dog), then class_mode would be 'binary'; else, it should be 'categorical'. As for the .flow_from_directory() command, it's actually something that's much more useful when the images (be it for the training or the testing datasets) are sorted into folders, accessing them directly and sorting them directly from those (by folder name, analogous to the name, "flow from directory").

2\. Describe the model architecture of the horses and humans CNN as you have specified it. Did you modify the number of filters in your Conv2D layers? How do image sizes decrease as they are passed from each of your Conv2D layers to your MaxPooling2D layer and on to the next iteration? Finally, which activation function have you selected for your output layer? What is the significance of this argument’s function within the context of your CNN’s prediction of whether an image is a horse or a human? What functions have you used in the arguments of your model compiler?

The model for the horses and humans CNN has the images pass through a Conv2D layer and then a MaxPooling2D layer, each 3 times, before being flattened before going into the two dense layers for its final classifications (similar to our DNN models!). Yes, I modified the number in my Conv2D layers a bit, to see what would happen. (In my code you will see that it's back in its "original" settings, though.) The way that Conv2D's image sizes get decreased is simple to explain, but the reason behind *why* is not so much. Simply, the edges get trimmed off by 1; this is because the program takes your input image with its color values and multiplies them by the filter (which is also 2D) to result in 1 number output, creating a brand new 2D array. It does this over and over. Logically speaking, this means that the pixels on the very edge of the image will slowly be "shaved" off each time, because each can only be used half the amount of times the other ones can be used. (This is actually true and conforms to the size of your filter; try it out yourself! You can see it by running a print statement to activate only when a certain cell, such as the edge cell, is used.) On the other hand, the MaxPooling2D layer essentially halves the lengths of the image, effectively quartering the image size. For my output layer, I used 'sigmoid'. 'sigmoid' is good for when there are only 2 possible outcomes, setting whatever the value was to either 0 or 1 (because it's binary). In the CNN's definition of horse or a human, when the classification is above 0.5, it's a human, and otherwise (anything below) is a horse. For my model compiler, I have used 'binary_crossentropy' for loss, RMSprop(lr=0.001) for optimizer, and 'accuracy' for metrics.

B. Regression

1\. Using the auto-mpg dataset (auto-mpg.data), upload the image where you used the seaborn library to pairwise plot the four variables specified in your model. Describe how you could use this plot to investigate the co-relationship amongst each of your variables. Are you able to identify interactions amongst variables with this plot? What does the diagonal access represent? Explain what this function is describing with regarding to each of the variables.

![image010](https://github.com/dshuangg/responses/raw/master/image010.png)

I can use this plot to see how any two variables correlate with each other. As a side note, I think that we can actually build a function (unless there is already one built into Python, which, I believe there is, maybe in sklearn's package?) to find the R score (or the R-squared score!) between any two variables. But otherwise, these plots allow us to *visibly* see the correlation, be it linear, quadratic, or even logarithmic/exponential. Yes, this allows us to easily identify interactions amongst variables--we can even standardize certain or all variables and run it again. The diagonal access would actually be when the variable would be plotted against itself, creating a direct y=x linear graph. In this case, it often shows a univariate distribution (but you can change it, I believe), which shows you how strongly correlated each variable is to the others. This function describes how useful each variable can be to determine another variable. For example, if the first variable and second variable have an R score of +.99, then you could pretty much almost guarantee that when the first variable is greater/larger, the second variable will be just as greatly/largely correlated! (Likewise if one is smaller, the other will be too.)

2\. After running model.fit() on the auto-mpg.data data object, you returned the hist.tail() from the dataset where the training loss, MAE & MSE were recorded as well as those same variables for the validating dataset. What interpretation can you offer when considering these last 5 observations from the model output? Does the model continue to improve even during each of these last 5 steps? Can you include a plot to illustrate your answer? Stretch goal: include and describe the final plot that illustrates the trend of true values to predicted values as overlayed upon the histogram of prediction error.

b2 ans

