
C. Word Embeddings

1\. Why is using one-hot encoding an inefficient towards vectorizing a corpus of words? How are word embeddings different? (see this video https:// www.youtube.com/watch?v=EEk6OiOOT2c )

In a one-hot encoding, all (most) of the values are 0's, except one 1, which indicates the represented value. Only one of these bits can be "hot" or true. As a result, corpuses of words will have one out of tens/hundreds/thousands/millions of 0's, with only one 1, taking up a lot of space and not being as efficient. On the other hand, the word embeddings are dense types. They use a different type of encoding, which is not necessarily binary, with 0's and 1's, and are much more computationally efficient. With word embeddings, they can have more data stored in them, as well, to tell the relation with other words, and especially within sentences or as groupings (hence, having different values than just a binary 0-or-1 system). The parameters are also trainable by the model/program itself. Something interesting to note is that if words have similar encodings, then they may have similar meanings!

As a result, since one-hot encodings are better for vectorizing categorical features, it doesn't work as well for millions of different types of unique words where they can be stored in a more compact method, with large amounts.

2\. Compile and train the model from the tensorflow exercise. Plot the training and validation loss as well as accuracy. Post your plots and describe them.

![image037](https://github.com/dshuangg/responses/raw/master/image037.png)
![image038](https://github.com/dshuangg/responses/raw/master/image038.png)

The validation loss seems to decrease until 3 epochs. Afterwards, the validation loss seems to steadily increase, worsening the model. The validation accuracy seems to be staying relatively steady over the 10 epochs, though looks like it may have a slight dip if the number of epochs is increased. (And as expected, the training loss decreases steadily while the training accuracy increases, as if the model were becoming better every epoch by getting more accurate, but is actually only identifying details that are specific to the provided dataset and not actually helpful.) The model is pretty decent, having a validation accuracy between 0.8 and 0.9.

D. Text Classification with an RNN

1\. Again compile and train the model from the tensorflow exercise. Plot the training and validation loss as well as accuracy. Stack two or more LSTM layers in your model. Post your plots and describe them.

![image039](https://github.com/dshuangg/responses/raw/master/image039.png)
![image040](https://github.com/dshuangg/responses/raw/master/image040.png)

The LSTM actually doesn't deviate much from without the LSTM. At least, I can't find any noticeable differences; only a couple values are varied in the training and validation values, but not off by very much, and most of the values remain in the same range. Similar to the loss and accuracy in the word embeddings assignment, the model becomes visibly overfit in the loss and less visibly so (but still slightly noticeable) in the accuracy. The model, overall, doesn't seem that bad as the accuracy is consistently between 0.8 and 0.9 (although the training keeps getting better and better even after the first epoch, henceforth explaining its overfitness).
