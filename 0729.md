
A. Using NLP to build a sarcasm classifier

1\. Pick two or three news sources and select a few news titles from their feed (about 5 is likely enough). For example you could select CNN, Fox News, MSNBC, NPR, PBS, Al Jazeera, RT (Russia Today), Deutsche Welle, Facebook, BBC, France24, CCTV, NHK World or another source you wish you analyze. Run your sarcasm model to predict whether the titles are interpreted as sarcastic or not. Analyze the results and comment on the different news sources you have selected.

```
['Tweets and Articles Sent Kodak Shares Surging Before Official Announcement', 'South Korean authorities encourage men to marry foreign women. But their brides often become victims of abuse', 'With remote back-to-school, child care challenges for providers, families emerge'] 
 [[3.0856918e-11]
 [7.9054689e-01]
 [1.2971806e-05]]
```

I've selected articles from Wall Street Journal, CNN, and USA Today (respectively) to analyze. For simplicity's sake, *here are only a few of the articles (that I picked) to get a representation of the accuracy of the model*. I've used a couple other headlines that were similarly identified as accurate and sarcastic, but were understandably wrong, similar to how I explain below.

The first and third article headlines were identified to not be sarcastic, whereas the second headline was identified as sarcastic. I can definitely understand why; the title seems long and even sound funny--why would the government encourage men to marry foreign women? Sounds like something that wouldn't happen, but the article simply talks about this in its article (seriously; not as a \[sarcastic\] joke). While a 2/3 (66%) score doesn't sound very good, it's reasonable why it got the second one wrong, so I would still evaluate the model as still decently accurate.

B. Text generation with an RNN

1\. Use the generate_text() command at the end of the exercise to produce synthetic output from your RNN model. Run it a second time and review the output. How has your RNN model been able to “learn” and “remember” the shakespeare text in order to reproduce a similar output?

```
ROMEO: Signior But within, tide, sir.

KING RICHARD III:
Northumberland for his love.

PETRUCHIO:
Manimyer your fellow-whose early blood, the fair goes bitterly

Nurse:
Passlar is signified at house of breast,
And title, sir, alans, we'th begun.

CORIOLANUS:
So pleaseth not
The steal honour here York thus trules,
The young'st our sweators I have bourde,
And we will appear too, and a grost and looth a thee:
The Bauntil, Kate, 'I'll not be means to mover.

PARIS:
Tish wash bringing salt, are they
ill tinistern your death:
Being heavens, or else perface these royalties:
And indeed his foul a spiritly revols more careful. Why sents thou
The nights shum wash thee, all mears I parted.

CLIFFORD:
To save a year as a?
I charge him, and so his followers.

CAMILLO:
I mean, I cannot play the e tooth, honest weaks this island's gear,
That must take thy murder of my lord,
Your honour is thy leave: I amvanter
Your farewell? 'He did stamply son, he does for many
Which have y a kind of scoper and unbold,
And
```

```
ROMEO: throat no rament then,
When you have under faults!

MENENIUS:
I'll have earth.

PETER: a ghost,
That, in his new rail't purdon in the earth,
Acoid such as is his reble inteeritors.

KING EDWARD IV:
St weep:
From whose cholarial ance in your son and fines
Had need Buckingham Biancil:
How villain, I'll nd either master, as our so offendam
What say so heavy hath made given:
A sacro myself are bride, with all
Be read.

Sirsh:
What art thou the city, if we may.
But whet men shall be mistress? and now my heart's neck is seem.
Mark me, the lord his limbship accursed him to yourselves.
Take heeds are magic,
That Anciusl he conceives for you.

ABRIOLAND:
O, the secrecors! I am no less:
Bashall'd will harm thy soul in a bawd turn, and my friends!

RIVERS:
She was a glush fallen well I grieve it shed
To dark so unlaim'd and die.
A cup of you as if I would were friar.

KING RICHARD TII:
We'll send thee well to tell the world:
incensequitants will not hope deliver'd,
Bemind in a word with life,
He'
```

My RNN model learns how each of the letters correspond with those letters before and after it, and generates words and sentences as such. The way that it generates each character that follows depends on the characters before it, as there are patterns that are stored from the model from its training in dense data. This data that it stores allows it to "remember" Shakespeare's writing style, although it doesn't make much sense (and I personally think that Shakespeare's writing was already difficult to understand in the first place). Here we can see that the two generations are different, since the choosing and the probabilities of each character is varied between generations so difficult results are produced.

C. Neural machine translation with attention

1\. Use the translate() command at the end of the exercise to translate three sentences from Spanish to English. How did your translations turn out?

hi

```
Tengo dos gatos.
I have two cats.

Estoy al lado del gran arbol.
I am next to the large tree.

Ahora miro un video de YouTube sobre una empresa holandesa antigua.
I am now watching a YouTube video about an old Dutch company.
```
