
A. Using NLP to build a sarcasm classifier

1\. Pick two or three news sources and select a few news titles from their feed (about 5 is likely enough). For example you could select CNN, Fox News, MSNBC, NPR, PBS, Al Jazeera, RT (Russia Today), Deutsche Welle, Facebook, BBC, France24, CCTV, NHK World or another source you wish you analyze. Run your sarcasm model to predict whether the titles are interpreted as sarcastic or not. Analyze the results and comment on the different news sources you have selected.

```
['Tweets and Articles Sent Kodak Shares Surging Before Official Announcement', 'South Korean authorities encourage men to marry foreign women. But their brides often become victims of abuse', 'With remote back-to-school, child care challenges for providers, families emerge'] 
 [[3.0856918e-11]
 [7.9054689e-01]
 [1.2971806e-05]]
```

I've selected articles from Wall Street Journal, CNN, and USA Today (respectively) to analyze. For simplicity's sake, *here are only a few of the articles (that I picked) to get a representation of the accuracy of the model*. I've used a couple other headlines that were similarly identified as accurate and sarcastic, but were understandably wrong, similar to how I explain below.

The first and third article headlines were identified to not be sarcastic, whereas the second headline was identified as sarcastic. I can definitely understand why; the title seems long and even sound funny--why would the government encourage men to marry foreign women? Sounds like something that wouldn't happen, but the article simply talks about this in its article (seriously; not as a \[sarcastic\] joke). While a 2/3 (66%) score doesn't sound very good, it's reasonable why it got the second one wrong, so I would still evaluate the model as still decently accurate.

B. Text generation with an RNN

1\. Use the generate_text() command at the end of the exercise to produce synthetic output from your RNN model. Run it a second time and review the output. How has your RNN model been able to “learn” and “remember” the shakespeare text in order to reproduce a similar output?

```
ROMEO: your face. Now is all men add,
Yet not so perchance, we should say:
Yet to since you have come in safety; for you send faced
'Barreaker: yet to Henry the poor whom betome
Whom for post to your wife and these news your heart.

YORK:
Why, stop theirs. Say I serve
Did give your ancient each man tog weak,
Besides to entreated me leis: be enough.

RATCLIFF:
Repast when it said, knot,---

POLIXENES:
Prithee.

VIRGILIA:
Is not; but lets as fone her; You're sprife
Or bare-couched to the colours like
A witners between the grace:
A mell me on thy body: and
craft's the story-counted shame to all the barth
Her wit your succession, Montague, but as she
will not long citizen:
Be prettied to the king.

GRUMIO:
Poor marketh, the meaning of those whopes!
My jord, did not be son,
If you'll chops thy lips with anon. O, sittle descent,
Who'st my others, and his court have got death?
His name is been!

Nurse:
O Prince not lick that in Gromio's butchers would seven
him, good God's grave in quarrels, as thou
```

```
ROMEO: they are in the to?
Who est yis shord I since, which you should sore
we not to reduce them not.

DORCAS:
What hair,--Horting the him,
He cannot know that I, the old hasty contract, like an omition.

BUCKINGHAM:
I dod affect thee shall we heaven and thy sister.

JULIET:
I desire England? who they shall
come and twenty vile ta's have heard
Without assediful the winders,
Who sitting Pe.llow nature, were it to be;
And where I send your knot to comfors thee,
What reins my mistress, that open their words!
What is't shall I to Boling and Angelo;
Whis to rise unto the issues of the prince haste
That ribs the courcious weaked take one word.

ELBOW:
Come, think you, letters; but you have found in,
That redecting when were my sorthum len than we bid that shall die:
You have nothing
same incarciunters and a-done; and e no time
Of your appresonvection have it
promptate. What nature, bechal youth,
Though we shall have forships.

CAPULET:
A good master, was dirttude,
As thousanks, cannot this naile t
```

My RNN model learns how each of the letters correspond with those letters before and after it, and generates words and sentences as such. The way that it generates each character that follows depends on the characters before it, as there are patterns that are stored from the model from its training in dense data. This data that it stores allows it to "remember" Shakespeare's writing style, although it doesn't make much sense (and I personally think that Shakespeare's writing was already difficult to understand in the first place). Here we can see that the two generations are different, since the choosing and the probabilities of each character is varied between generations so difficult results are produced.

C. Neural machine translation with attention

1\. Use the translate() command at the end of the exercise to translate three sentences from Spanish to English. How did your translations turn out?

```
Input: 
Tengo dos gatos.
Estoy al lado del gran arbol.
Ahora estoy mirando un video de YouTube sobre una empresa holandesa antigua.

Expected output (answers):
I have two cats.
I am next to the large tree.
Now I am watching a YouTube video about an old Dutch company.

Actual output:
i have two cats .
i m on your news .
KeyError: 'youtube'
```

It's not that good. It wasn't able to translate the second sentence correctly, even though it was relatively simple. It also doesn't understand what "YouTube" is, which, is just a proper noun. It's possible to make an exception to the rule, though--this would be able to account for many "special" cases by just outputting the exact word that was input as a proper noun (to not change anything to it). 

In order to (attempt) to avoid this, I replaced "YouTube" with "alguna cosa" (which means "something"). Yet, still, "KeyError: 'holandesa'". Unfortunate. I could remove "holandesa" as well, but this just goes to show how the program, at this stage, at least, cannot translate certain unique words such as adjectives or proper nouns. Otherwise, it can translate basic sentences like the ones provided in the example code, as well as my first sentence.

