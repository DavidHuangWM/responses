
A. Using NLP to build a sarcasm classifier

1. Pick two or three news sources and select a few news titles from their feed (about 5 is likely enough). For example you could select CNN, Fox News, MSNBC, NPR, PBS, Al Jazeera, RT (Russia Today), Deutsche Welle, Facebook, BBC, France24, CCTV, NHK World or another source you wish you analyze. Run your sarcasm model to predict whether the titles are interpreted as sarcastic or not. Analyze the results and comment on the different news sources you have selected.

```python
['Tweets and Articles Sent Kodak Shares Surging Before Official Announcement', 'South Korean authorities encourage men to marry foreign women. But their brides often become victims of abuse', 'With remote back-to-school, child care challenges for providers, families emerge'] 
 [[3.0856918e-11]
 [7.9054689e-01]
 [1.2971806e-05]]
```

I've selected articles from Wall Street Journal, CNN, and USA Today (respectively) to analyze. The first and third article headlines were identified to not be sarcastic, whereas the second headline was identified as sarcastic. I can definitely understand why; the title seems long and even sound funny--why would the government encourage men to marry foreign women? Sounds like something that wouldn't happen, but the article simply talks about this in its article (seriously; not as a \[sarcastic\] joke). While a 2/3 (66%) score doesn't sound very good, it's reasonable why it got the second one wrong, so I would still evaluate the model as still decently accurate.

B. Text generation with an RNN

1. Use the generate_text() command at the end of the exercise to produce synthetic output from your RNN model. Run it a second time and review the output. How has your RNN model been able to “learn” and “remember” the shakespeare text in order to reproduce a similar output?

hi

C. Neural machine translation with attention

1. Use the translate() command at the end of the exercise to translate three sentences from Spanish to English. How did your translations turn out?

hi

