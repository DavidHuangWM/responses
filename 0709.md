
1\. What is TF Hub? How did you use it when creating your script for “text classification of movie reviews”?

TF Hub is a library of machine learning models. These help the machine be able to train models more efficiently. I used it when processing the sentences on the movie reviews. Directly, this was used for initializing the keras layer to be used for the model.

2\. What are the optimizer and loss functions? How good was your “text classification of movie reviews” model?

The optimizer function used is 'adam', and the loss function is BinaryCrossentropy. My "text classification of movie reviews" model was pretty good--very close to 90%, but not quite. My model had 87% accuracy; precisely, the accuracy was 86.60%. This isn't perfect, but it's relatively "good" given its high accuracy.

3\. In “text classification with preprocessed text” you produced a graph of training and validation loss. Add the graph to this response and provide a brief explanation.

![image003](https://github.com/dshuangg/responses/raw/master/image003.png)

As the model goes through its iterations (epochs), the machine attempts to better model, specifically by lowering its loss by a process determined by the 'adam' optimizer. The validation loss actually gets higher, which means it's getting worse, but the training loss continues to become smaller (where a naive interpretation would recognize that as "getting better"). The model finds patterns in the training data that aren't necessarily present in the testing data.

4\. Likewise do the same for the training and validation accuracy graph.

![image004](https://github.com/dshuangg/responses/raw/master/image004.png)

Similar to the graph of training and validation loss, the training accuracy continuously gets better (higher values) at a seemingly smooth rate. The validation accuracy on my graph seems to be relatively stable, leveling off around 86% accuracy after 10-15 iterations (epochs), where the training accuracy still continues to increase.

