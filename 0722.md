
A. Boosted Trees

1\. What is a one-hot-encoded column and why might it be needed when transforming a feature? Are the source values continuous or discrete?

A one-hot-encoded column is one that has values of all 0 except one single 1, determining the coinciding correct label, effectively representing that class/category. These source values are discrete.

2\. What is a dense feature? For example, if you execute example = dict(dftrain) and then tf.keras.layers.DenseFeatures(your_features)(your_object).numpy(), how has the content of your data frame been transformed? Why might this be useful?

A dense feature converts all data from a specific point into an array to be processed further. Any blank entries of data is still recognized as data and is converted to 0's, where they can be useful for other layers of predictions.

3\. Provide a histogram of the probabilities for the logistic regression as well as your boosted tree model. How do you interpret the two different models? Are their predictions essentially the same or is there some area where they are noticeable different. Plot the probability density function of the resulting probability predictions from the two models and use them to further illustrate your argument. Include the ROC plot and interpret it with regard to the proportion of true to false positive rates, as well as the area under the ROC curve. How does the measure of the AUC reflect upon the predictive power of your model?

![image030](https://github.com/dshuangg/responses/raw/master/image030.png)
![image031](https://github.com/dshuangg/responses/raw/master/image031.png)
![image032](https://github.com/dshuangg/responses/raw/master/image032.png)

In the first (logistic regression) model, the probabilities for survival are more evenly distributed, "spread out." On the second (boosted tree) model, the probabilities are more clustered near 0 and 1. This could be showing how the weighted data becomes more prominent in determining factors such as on age and sex, and giving higher and lower, distinctly varying chances of survival. The ROC curve is pretty good! The true positive rate hits 0.8 when the false positive rate only hits 0.2, meaning that most of the predictions are correct, at a rate of approximately 4/5. The area under the curve is also indicative of the model being good, as a perfect model would have the area under the curve being the maximum amount. (Our model is almost the maximum amount, with the only less-than-ideal part being 0.0-0.2.)

B. Boosted Trees continued (with model understanding)

1\. Upload your feature values contribution to predicted probability horizontal bar plot as well as your violin plot. Interpret and discuss the two plots. Which features appear to contribute the most to the predicted probability?

hi

2\. Upload at least 2 feature importance plots. Which features are the most important in their contribution to your models predictive power?

hi

